# get some recent papers

import requests
import os
import time
import sys

################



search_categories = ['math.HO', 'math.GM', 'math.NT', 'cs.GT', 'cs.CC', 'cs.CR', 'eess.SY', 'eess.SP']
category_blacklist = ['math.AT', 'math.CT', 'math.GT', 'math.KT', 'math.OA', 'math.RT', 'math.SP']
max_papers = 7
base_link = 'https://export.arxiv.org/api/query'
term_length, term_height = 120, 30 # defaults for terminal width/height

##################


class InternetConnectivityException(Exception):
    pass


def has_digit(s):
    for i in s:
        if ord(i) > ord('0') - 1 and ord(i) < ord('9') + 1:
            return True
        
    return False 

def clear_scr():
    print('\033[2J\033[H')
    return 


def is_seen(x, y):
    # is date string x > y?


    # year
    if x[:4] > y[:4]:
        return True 

    if x[:4] < y[:4]:
        return False 
    
    
    # month
    if x[5:7] > y[5:7]:
        return True 
    
    if x[5:7] < y[5:7]:
        return False 
    
    # day
    if x[8:10] > y[8:10]:
        return True 


    
    return False 


# if is_seen(_last_date, paper_info[0]):
#            continue 

# parses the massive atom xml file generated by arxiv
# returns list of lists of [id, title, summary, author(s), category(s)] for each paper
# author(s)/category(s) will also be a list

def arxiv_atom_parser(atom_str):
    _new_str = atom_str.replace('\t', '')
    _lines = _new_str.split('\n')

    papers_info = []
    _temp_paper_info = []
    _entry = False
    _title = False
    _summary = False 
    _author = False 

    _title_s = ''
    _summary_s = ''
    _author_ls = []
    _categories = []


    for _idx in range(len(_lines)):
        _lines[_idx] = _lines[_idx].strip() # parsing
      
        # print(_lines[_idx])


        if '</author>' in _lines[_idx]:
            _author = False
            continue

        if '</entry>' in _lines[_idx]:
            # wrap up
            _entry = False
            _temp_paper_info.append(_author_ls)
            _temp_paper_info.append(_categories)
            _author_ls = []
            _categories = []

            _temp_paper_info[2] = _temp_paper_info[2].strip() # some weird spaces

            papers_info.append(_temp_paper_info)
            _temp_paper_info = []
            continue 

        if '<entry>' in _lines[_idx]:
            _entry = True
            continue 

        if '<published>' in _lines[_idx] and _entry: 
            # log date of publishing
            _date = _lines[_idx][11:-22]
            _temp_paper_info.insert(0, _date)
            continue 
        
        if '<title>' in _lines[_idx] and _entry: # might be one line, might be two
            if '</title>' in _lines[_idx]:
                _title_s = _lines[_idx][7:-8]
                _temp_paper_info.append(_title_s)
                _title_s = ''
                continue 

            _title = True # multi-line title
            _title_s = _lines[_idx][7:]
            continue 

        if _title:
            if '</title>' in _lines[_idx]:
                # end it here
                _title = False 
                _title_s += ' ' + _lines[_idx][:-8]
                _temp_paper_info.append(_title_s)
                _title_s = ''
                continue 

            # just continue on i guess
            _title_s += ' ' + _lines[_idx]
            continue 


        if '<summary>' in _lines[_idx] and _entry and '</summary>' in _lines[_idx]:
            _temp_paper_info.append(_lines[_idx][9:-10])
            continue 
        
        if '</summary>' in _lines[_idx]:
            _summary_s += _lines[_idx][:-10]
            _temp_paper_info.append(_summary_s)
            _summary_s = ''
            _summary = False 
            continue

        if _summary:
            _summary_s += _lines[_idx].strip()
            continue 

        if '<summary>' in _lines[_idx] and _entry:
            _summary = True 
            _summary_s += _lines[_idx][9:]
            continue 



        # author
        if '<author>' in _lines[_idx] and _entry:
            # only thing in the line, no need to do anything else
            _author = True
            continue 
        
        
        
        if _author:
            # assuming the name tag is in there
            _author_ls.append(_lines[_idx][6:-7])
            continue 


        # '<category term="math.NT" scheme="http://arxiv.org/schemas/atom"/>'

        if '<category' in _lines[_idx]:
            # farm terms
            _categories.append(_lines[_idx][16:].split('"')[0])

            if has_digit(_categories[-1]):
                _categories.pop() # not the arxiv categories
            continue 


        continue 

    return papers_info


########################################################

# preliminary work

papers_last_date = [] # (cat, date string)
# date string ~ 2025-12-30

# thanks python documentation
try:
    f = open('papers_seen.txt', 'r')
except FileNotFoundError:
    pass 
else:
    with f:
        for _l in f:
            _c, _v = _l.split() 
            papers_last_date.append((_c, _v))

    f.close()


for _cat in search_categories:

    _last_date = '1234.56789v1'
    for _i, _p in enumerate(papers_last_date):
        if _p[0] == _cat:
            _last_date = _p[1]
            papers_last_date.pop(_i)
            break
        continue 

    # wait for 3 + epsilon seconds as requested by arxiv
    clear_scr()
    print("Please wait - Loading next section.....")
    time.sleep(3.1415926)

    # NOTE: to encode a '+' in query, use a space!
    # see end of section 5.1: info.arxiv.org/help/api/user-manual.html#51-details-of-query-construction
    _search_params = {'search_query': f'cat:{_cat}', 'sortBy': 'submittedDate', 'sortOrder': 'descending', 'max_results': max_papers}

    try:
        _r = requests.get(base_link, params=_search_params)
    except ConnectionError or MaxRetryError or NameResolutionError:
        raise InternetConnectivityException("Are you connected to the internet?")
    except Exception as _e:
        print(f'Something is wrong with your arxiv request for parameters {_search_params} on category {_cat}.')
        print('See below for more information.')
        print()
        raise _e

    if _r.status_code != 200:
        print(f'Your Arxiv request for category {_cat} failed with status code {_r.status_code}.')
        print('Please try again later.')
        sys.exit(1)


    new_papers = arxiv_atom_parser(_r.text)




    for paper_info in new_papers:
        # [0] - ver/paper uid
        # [1] - title
        # [2] - summary
        # [3] - author(s) - list
        # [4] - category(s) - list 

        # check category blacklist
        if len(set(category_blacklist) & set(paper_info[4])) > 0:
            continue 

        # check if past the threshold date
        if is_seen(_last_date, paper_info[0]):
            continue 

        # display to user
        clear_scr()
        print(f'current category: {_cat}')
        print()
        print(f'{' '*((term_length - len(paper_info[1])) // 2)}{paper_info[1]}\n')
        print('\t' + paper_info[2] + '\n')

        resp = input(f'{'-'*(term_length // 4)}\n> ')

        if 'n' in resp:
            continue 

        if 'y' in resp:
            # open html in firefox (to download)
            os.system(f'start firefox "https://arxiv.org/pdf/{paper_info[0]}"')
            continue 

        # no response, don't care
        continue 
    
    if len(new_papers) == 0:
        papers_last_date.append((_cat, _last_date))
        continue 

    papers_last_date.append((_cat, new_papers[0][0]))
    continue 

try:
    os.remove('papers_seen.txt')
except FileNotFoundError:
    pass # doesn't matter

with open('papers_seen.txt', 'w') as f:
    for pair in papers_last_date:
        f.write(f'{pair[0]} {pair[1]}\n')


clear_scr()
print('done!')