# get some recent papers

import requests
import os
import time
import sys

################

# removed: 'cs.DS'

search_categories = ['math.HO', 'math.GM', 'cs.GT', 'cs.CC', 'cs.CR', 'math.NT', 'eess.SP', 'eess.SY']
category_blacklist = ['math.AT', 'math.CT', 'math.GT', 'math.KT', 'math.OA', 'math.RT', 'math.SP']
max_papers = 7
base_link = 'https://export.arxiv.org/api/query'
term_length, term_height = 120, 30 # defaults for terminal width/height

##################


class InternetConnectivityException(Exception):
    pass


def has_digit(s):
    for i in s:
        if ord(i) > ord('0') - 1 and ord(i) < ord('9') + 1:
            return True
        
    return False 

# parses the massive atom xml file generated by arxiv
# returns list of lists of [id, title, summary, author(s), category(s)] for each paper
# author(s)/category(s) will also be a list
def arxiv_atom_parser(atom_str):
    _new_str = atom_str.replace('\t', '')
    _lines = atom_str.split('\n')

    papers_info = []
    _temp_paper_info = []
    _entry = False
    _title = False
    _title_s = ''
    _summary = False
    _summary_s = ''
    _author = False
    _author_ls = []
    _categories = []

    for _idx in range(len(_lines)):
        _lines[_idx] = _lines[_idx].strip() # parsing
      

        if '<entry>' in _lines[_idx]:
            _entry = True
            continue 

        if '<id>' in _lines[_idx] and _entry: # 2nd conditional to filter out the first entry in the api call
            # log doi
            _doi = _lines[_idx][25:-5]
            _temp_paper_info.append(_doi)
            continue 

        if '<id>' in _lines[_idx]:
            continue # idk why ur here buddy
        
        if '<title>' in _lines[_idx] and _entry: # might be one line, might be two
            if '</title>' in _lines[_idx]:
                _title_s = _lines[_idx][7:-8]
                _temp_paper_info.append(_title_s)
                _title_s = ''
                continue 

            _title = True # multi-line title
            _title_s = _lines[_idx][7:]
            continue 

        if _title:
            if '</title>' in _lines[_idx]:
                # end it here
                _title = False 
                _title_s += ' ' + _lines[_idx][:-8]
                _temp_paper_info.append(_title_s)
                _title_s = ''
                continue 

            # just continue on i guess
            _title_s += ' ' + _lines[_idx]
            continue 


        # do the same thing with summary
        if '<summary>' in _lines[_idx] and _entry: # at least one line
            _summary = True # multi-line title
            _summary_s = _lines[_idx][9:]
            continue 

        if _summary and '</summary>' not in _lines[_idx]:
            # continue parsing
            _summary_s += ' ' + _lines[_idx]
            continue 

        if '</summary>' in _lines[_idx]:
            # no text here since it seems this its only line 
            # just add everything in and wrap up
            _summary = False
            _temp_paper_info.append(_summary_s)
            _summary_s = ''
            continue 

        
        # author
        if '<author>' in _lines[_idx] and _entry:
            # only thing in the line, no need to do anything else
            _author = True
            continue 
        
        if '</author>' in _lines[_idx]:
            _author = False
            continue
        
        if _author:
            # assuming the name tag is in there
            _author_ls.append(_lines[_idx][6:-7])
            continue 

        if '</entry>' in _lines[_idx]:
            # wrap up
            _entry = False
            _temp_paper_info.append(_author_ls)
            _temp_paper_info.append(_categories)
            _author_ls = []
            _categories = []

            _temp_paper_info[2] = _temp_paper_info[2].strip() # some weird spaces

            papers_info.append(_temp_paper_info)
            _temp_paper_info = []
            continue 

        # '<category term="math.NT" scheme="http://arxiv.org/schemas/atom"/>'

        if '<category' in _lines[_idx]:
            # farm terms
            _categories.append(_lines[_idx][16:].split('"')[0])

            if has_digit(_categories[-1]):
                _categories.pop() # not the arxiv categories
            continue 


        continue 
    
    return papers_info


def clear_scr():
    print('\033[2J\033[H')
    return 

################

# preliminary work

papers_seen = [] # (cat, id)

# thanks python documentation
try:
    f = open('papers_seen.txt', 'r')
except FileNotFoundError:
    pass 
else:
    with f:
        for _l in f:
            _c, _v = _l.split() # lexigraphically compare strings
            papers_seen.append((_c, _v))

    f.close()


for _cat in search_categories:

    _last_seen = '1234.56789v1'
    for _i, _p in enumerate(papers_seen):
        if _p[0] == _cat:
            _last_seen = _p[1]
            papers_seen.pop(_i)
            break
        continue 

    # wait for 3 + epsilon seconds as requested by arxiv
    clear_scr()
    print("Please wait - Loading next section.....")
    time.sleep(3.1415926)

    # NOTE: to encode a '+' in query, use a space!
    # see end of section 5.1: info.arxiv.org/help/api/user-manual.html#51-details-of-query-construction
    _search_params = {'search_query': f'cat:{_cat}', 'sortBy': 'submittedDate', 'sortOrder': 'descending', 'max_results': max_papers}

    try:
        _r = requests.get(base_link, params=_search_params)
    # TODO -- im not sure if this is valid code to catch all exceptions
    except ConnectionError or MaxRetryError or NameResolutionError:
        raise InternetConnectivityException("Are you connected to the internet?")
    except Exception as _e:
        print(f'Something is wrong with your arxiv request for parameters {_search_params} on category {_cat}.')
        print('See below for more information.')
        print()
        raise _e

    if _r.status_code != 200:
        print(f'Your Arxiv request for category {_cat} failed with status code {_r.status_code}.')
        print('Please try again later.')
        sys.exit(1)

    new_papers = arxiv_atom_parser(_r.text)

    for paper_info in new_papers:
        # [0] - ver/paper uid
        # [1] - title
        # [2] - summary
        # [3] - author(s) - list
        # [4] - category(s) - list 

        # check category blacklist
        if len(set(category_blacklist) & set(paper_info[4])) > 0:
            continue 

        # check if seen
        if paper_info[0] <= _last_seen:
            continue 

        # display to user
        clear_scr()
        print(f'current category: {_cat}')
        print()
        print(f'{' '*((term_length - len(paper_info[1])) // 2)}{paper_info[1]}\n')
        print('\t' + paper_info[2] + '\n')

        resp = input(f'{'-'*(term_length // 4)}\n> ')

        if 'n' in resp:
            continue 

        if 'y' in resp:
            # open html in firefox (to download)
            os.system(f'start firefox "https://arxiv.org/pdf/{paper_info[0]}"')
            continue 

        # no response, don't care
        continue 
    
    if len(new_papers) == 0:
        papers_seen.append((_cat, _last_seen))
        continue 

    papers_seen.append((_cat, new_papers[0][0]))
    continue 

try:
    os.remove('papers_seen.txt')
except FileNotFoundError:
    pass # doesn't matter

with open('papers_seen.txt', 'w') as f:
    for pair in papers_seen:
        f.write(f'{pair[0]} {pair[1]}\n')


clear_scr()
print('done!')